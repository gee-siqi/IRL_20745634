# -*- coding: utf-8 -*-
"""Assignment (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/187NJm2aTnCt3ij6WR7uIfFbSYovva2x0

# Assignment
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/My Drive/Colab Notebooks/irl')

# Import 

import numpy as np
import matplotlib.pyplot as plt
from degree_freedom_queen import *
from degree_freedom_king1 import *
from degree_freedom_king2 import *
from generate_game import *
from Chess_env import *

size_board = 4

"""## The Environment

You can find the environment in the file Chess_env, which contains the class Chess_env. To define an object, you need to provide the board size considered as input. In our example, size_board=4. 
Chess_env is composed by the following methods:

1. Initialise_game. The method initialises an episode by placing the three pieces considered (Agent's king and queen, enemy's king) in the chess board. The outputs of the method are described below in order.

     S $\;$ A matrix representing the board locations filled with 4 numbers: 0, no piece in that position; 1, location of the 
     agent's king; 2 location of the queen; 3 location of the enemy king.
     
     X $\;$ The features, that is the input to the neural network. See the assignment for more information regarding the            definition of the features adopted. To personalise this, go into the Features method of the class Chess_env() and change        accordingly.
     
     allowed_a $\;$ The allowed actions that the agent can make. The agent is moving a king, with a total number of 8                possible actions, and a queen, with a total number of $(board_{size}-1)\times 8$ actions. The total number of possible actions correspond      to the sum of the two, but not all actions are allowed in a given position (movements to locations outside the borders or      against chess rules). Thus, the variable allowed_a is a vector that is one (zero) for an action that the agent can (can't)      make. Be careful, apply the policy considered on the actions that are allowed only.
     

2. OneStep. The method performs a one step update of the system. Given as input the action selected by the agent, it updates the chess board by performing that action and the response of the enemy king (which is a random allowed action in the settings considered). The first three outputs are the same as for the Initialise_game method, but the variables are computed for the position reached after the update of the system. The fourth and fifth outputs are:

     R $\;$ The reward. To change this, look at the OneStep method of the class where the rewards are set.
     
     Done $\;$ A variable that is 1 if the episode has ended (checkmate or draw).
     
     
3. Features. Given the chessboard position, the method computes the features.

This information and a quick analysis of the class should be all you need to get going. The other functions that the class exploits are uncommented and constitute an example on how not to write a python code. You can take a look at them if you want, but it is not necessary.





"""

## INITIALISE THE ENVIRONMENT

env=Chess_Env(size_board)

## PRINT 5 STEPS OF AN EPISODE CONSIDERING A RANDOM AGENT

S,X,allowed_a=env.Initialise_game()                       # INTIALISE GAME

print(S)                                                  # PRINT CHESS BOARD (SEE THE DESCRIPTION ABOVE)

print('check? ',env.check)                                # PRINT VARIABLE THAT TELLS IF ENEMY KING IS IN CHECK (1) OR NOT (0)
print('dofk2 ',np.sum(env.dfk2_constrain).astype(int))    # PRINT THE NUMBER OF LOCATIONS THAT THE ENEMY KING CAN MOVE TO


for i in range(5):
    
    a,_=np.where(allowed_a==1)                  # FIND WHAT THE ALLOWED ACTIONS ARE
    a_agent=np.random.permutation(a)[0]         # MAKE A RANDOM ACTION

    S,X,allowed_a,R,Done=env.OneStep(a_agent)   # UPDATE THE ENVIRONMENT
    
    
    ## PRINT CHESS BOARD AND VARIABLES
    print('')
    print(X)
    print(S)
    #print(R,'', Done)
    print(len(allowed_a))
    print('check? ',env.check)
    print('dofk2 ',np.sum(env.dfk2_constrain).astype(int))
    
    
    # TERMINATE THE EPISODE IF Done=True (DRAW OR CHECKMATE)
    if Done:
        break

# PERFORM N_episodes=1000 EPISODES MAKING RANDOM ACTIONS AND COMPUTE THE AVERAGE REWARD AND NUMBER OF MOVES 

S,X,allowed_a=env.Initialise_game()
N_episodes=1000

# VARIABLES WHERE TO SAVE THE FINAL REWARD IN AN EPISODE AND THE NUMBER OF MOVES 
R_save_random = np.zeros([N_episodes, 1])
N_moves_save_random = np.zeros([N_episodes, 1])

for n in range(N_episodes):

    S,X,allowed_a=env.Initialise_game()     # INITIALISE GAME
    Done=0                                  # SET Done=0 AT THE BEGINNING
    i=1                                     # COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE
    
    # UNTIL THE EPISODE IS NOT OVER...(Done=0)
    while Done==0:
        
        # SAME AS THE CELL BEFORE, BUT SAVING THE RESULTS WHEN THE EPISODE TERMINATES 
        
        a,_=np.where(allowed_a==1)
        a_agent=np.random.permutation(a)[0]

        S,X,allowed_a,R,Done=env.OneStep(a_agent)
        
        
        if Done:
            
            R_save_random[n]=np.copy(R)
            N_moves_save_random[n]=np.copy(i)

            break

        i=i+1                               # UPDATE THE COUNTER



# AS YOU SEE, THE PERFORMANCE OF A RANDOM AGENT ARE NOT GREAT, SINCE THE MAJORITY OF THE POSITIONS END WITH A DRAW 
# (THE ENEMY KING IS NOT IN CHECK AND CAN'T MOVE)

print('Random_Agent, Average reward:',np.mean(R_save_random),'Number of steps: ',np.mean(N_moves_save_random))



"""## Initialization"""

# INITIALISE THE PARAMETERS OF YOUR NEURAL NETWORK AND...
# PLEASE CONSIDER TO USE A MASK OF ONE FOR THE ACTION MADE AND ZERO OTHERWISE IF YOU ARE NOT USING VANILLA GRADIENT DESCENT...
# WE SUGGEST A NETWORK WITH ONE HIDDEN LAYER WITH SIZE 200. 
np.random.seed(20)  ## Set the random seed

S,X,allowed_a=env.Initialise_game()

N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS

N_in=np.shape(X)[0]    ## INPUT SIZE
N_h=200                ## NUMBER OF HIDDEN NODES


## INITALISE YOUR NEURAL NETWORK...
W1 = np.random.randn(N_h, N_in) * np.sqrt(1/N_in)
W2 = np.random.randn(N_a, N_h) * np.sqrt(1/N_h)

# Initialize the biases
bias_W1 = np.zeros((N_h,))
bias_W2 = np.zeros((N_a,))

# Initialize the target nueral network with the same way
W1_tar = np.random.randn(N_h, N_in) * np.sqrt(1 / (N_in))
W2_tar = np.random.randn(N_a, N_h) * np.sqrt(1 / (N_h))
bias_W1_tar = np.zeros((N_h,))
bias_W2_tar = np.zeros((N_a,))


# HYPERPARAMETERS SUGGESTED (FOR A GRID SIZE OF 4)

epsilon_0 = 0.2     # STARTING VALUE OF EPSILON FOR THE EPSILON-GREEDY POLICY
beta = 0.00005      # THE PARAMETER SETS HOW QUICKLY THE VALUE OF EPSILON IS DECAYING (SEE epsilon_f BELOW)
gamma = 0.85        # THE DISCOUNT FACTOR
eta = 0.0035        # THE LEARNING RATE

N_episodes = 100000 # THE NUMBER OF GAMES TO BE PLAYED 

# SAVING VARIABLES
R_save_Q = np.zeros([N_episodes, 1])
N_moves_save_Q = np.zeros([N_episodes, 1])

#epsilon greedy policy(for greedy policy: epsilon == 1)
# Random array to decide whether the choice is greedy or random
rand_array = np.random.rand(N_episodes+10000)

def epsilon_greedy(epsilon, allowed_a, x, idx):
      a,_=np.where(allowed_a==1)
      if epsilon > rand_array[idx]:
          #n%len(a) make sure even facing a same allowed action set, the choice is still random
          a_agent=np.random.permutation(a)[idx%len(a)]  
      else:
          #select allowed actions
          a_a = np.zeros([32, ])
          for j in a:
              a_a[j] = x[j] # Only keep q values of allowed actions 
          a_agent = np.argmax(a_a)
      return a_agent

# Then epsilon_greedy(0, allowed_a, x, 0) would be the greedy policy

"""## Q-Learning"""

for n in range(N_episodes):
    
    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON
    Done=0                                   ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)
    i = 1                                    ## COUNTER FOR NUMBER OF ACTIONS
    
    S,X,allowed_a=env.Initialise_game()      ## INITIALISE GAME
    
    while Done == 0:
                # Initialise the gradients for each batch
        dW1 = np.zeros(W1.shape)
        dW2 = np.zeros(W2.shape)

        dbias_W1 = np.zeros(bias_W1.shape)
        dbias_W2 = np.zeros(bias_W2.shape)
        
        # Input = X, output = x2
        #nueral activation: input layer to hidden layer
        h1 = np.dot(W1, X) + bias_W1
        #sigmoid
        x1 = 1/(1+np.exp(-h1))
        #nueral activation: hidden layer to output layer
        h2 = np.dot(W2, x1) + bias_W2
        #sigmoid
        x2 = 1/(1+np.exp(-h2))
        #print(x2) 
        
        # Use epsilon-greedy policy to choose an action
        #Set index as i+N to make sure each epsilon greedy policy is random
        a = epsilon_greedy(epsilon_f, allowed_a, x2, i+n)

        # Make an action
        S_next,X_next,allowed_a_next,R,Done=env.OneStep(a)

        # One hot code with action index = 1 as mask
        mask = np.zeros((N_a, ))
        mask[a] = 1  
        
        if Done == 1:
            Q_bar = x2[a] #predicted Q value
            #calculate the error signal
            e_n = R - Q_bar
            
            # Backpropagation: output layer -> hidden layer
            delta2 = x2 * (1-x2) * e_n * mask
            
            dW2 += np.outer(delta2, x1)
            dbias_W2 += delta2
            
            # Backpropagation: hidden layer -> input layer
            delta1 = x1 * (1-x1) * np.dot(W2.T, delta2)
            dW1 += np.outer(delta1,X)
            dbias_W1 += delta1
            
            W1 += eta * dW1
            W2 += eta * dW2
            bias_W1 += eta * dbias_W1
            bias_W2 += eta * dbias_W2
            
            R_save_Q[n] = np.copy(R) #saving variables
            N_moves_save_Q[n] = np.copy(i)
            print('episoid:',n+1,'; R:',R,'; Number of steps: ',i)
            break
            
            
        else:
            #update step number
            i += 1
          
            # Same neraul activation
            h1_tar = np.dot(W1, X_next) + bias_W1
            x1_tar = 1/(1+np.exp(-h1_tar))
            h2_tar = np.dot(W2, h1_tar) + bias_W2
            x2_tar = 1/(1+np.exp(-h2_tar))
            
            #use greedy policy to find action at S'
            a2 = epsilon_greedy(0, allowed_a_next, x2_tar, 0)
            
            # Compute the error signal
            Q_bar = x2[a]
            Q_tar = R + gamma * x2_tar[a2] 
            e_n = Q_tar - Q_bar

            # Backpropagation: output layer -> hidden layer
            delta2 = x2 * (1-x2) * e_n * mask
            
            dW2 += np.outer(delta2, x1)
            dbias_W2 += delta2
            
            # Backpropagation: hidden layer -> input layer
            delta1 = x1*(1-x1) * np.dot(W2.T, delta2)
            dW1 += np.outer(delta1,X_next)
            dbias_W1 += delta1
            
            W1 += eta * dW1
            W2 += eta * dW2
            bias_W1 += eta * dbias_W1
            bias_W2 += eta * dbias_W2
            
            
        # Pass state values to the next step
        S = np.copy(S_next)
        X = np.copy(X_next)
        allowed_a = np.copy(allowed_a_next)

print('Q-Learning, Average reward:',np.mean(R_save_Q),'Number of steps: ',np.mean(N_moves_save_Q))

#exponential moving average
alpha = 100/N_episodes
R_EMA_Q = np.zeros((N_episodes, ))
steps_EMA_Q = np.zeros((N_episodes, ))
for i in range(N_episodes):
  if i == 0:
    R_EMA_Q[i] = R_save_Q[i]
    steps_EMA_Q[i] = N_moves_save_Q[i]

  else:
    R_EMA_Q[i] = alpha * R_save_Q[i] + (1 - alpha) * R_EMA_Q[i-1]
    steps_EMA_Q[i] = alpha * N_moves_save_Q[i] + (1 - alpha) * steps_EMA_Q[i-1]



plt.plot(R_EMA_Q)
plt.xlabel('Episodes')
plt.ylabel('Reward')
plt.title('EMA of Rewards')
plt.show()

plt.plot(steps_EMA_Q)
plt.xlabel('Episodes')
plt.ylabel('Steps')
plt.title('EMA of Steps')
plt.show()

"""## Change gamma and beta

Gamma is the discount factor
beta is the decay factor of epsilon.

If gamma

## SARSA
"""

## Initialize the parameters for SARSA
## INITALISE YOUR NEURAL NETWORK...
W1 = np.random.randn(N_h, N_in) * np.sqrt(1/N_in)
W2 = np.random.randn(N_a, N_h) * np.sqrt(1/N_h)

# Initialize the biases
bias_W1 = np.zeros((N_h,))
bias_W2 = np.zeros((N_a,))

# Initialize the target nueral #network with the same way
W1_tar = np.random.randn(N_h, N_in) * np.sqrt(1 / (N_in))
W2_tar = np.random.randn(N_a, N_h) * np.sqrt(1 / (N_h))
bias_W1_tar = np.zeros((N_h,))
bias_W2_tar = np.zeros((N_a,))

# SAVING VARIABLES
R_save_SARSA = np.zeros([N_episodes, 1])
N_moves_save_SARSA = np.zeros([N_episodes, 1])

for n in range(N_episodes):
    
    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON
    Done=0                                   ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)
    i = 1                                    ## COUNTER FOR NUMBER OF ACTIONS
    
    S,X,allowed_a=env.Initialise_game()      ## INITIALISE GAME
    
    while Done == 0:
                # Initialise the gradients for each batch
        dW1 = np.zeros(W1.shape)
        dW2 = np.zeros(W2.shape)

        dbias_W1 = np.zeros(bias_W1.shape)
        dbias_W2 = np.zeros(bias_W2.shape)
        
        # Input = X, output = x2
        #nueral activation: input layer to hidden layer
        h1 = np.dot(W1, X) + bias_W1
        #sigmoid
        x1 = 1/(1+np.exp(-h1))
        #nueral activation: hidden layer to output layer
        h2 = np.dot(W2, x1) + bias_W2
        #sigmoid
        x2 = 1/(1+np.exp(-h2))
        #print(x2) 
        
        # Use epsilon-greedy policy to choose an action
        a = epsilon_greedy(epsilon_f, allowed_a, x2)

        # Make an action
        S_next,X_next,allowed_a_next,R,Done=env.OneStep(a)

        # One hot code with action index = 1 as mask
        mask = np.zeros((N_a, ))
        mask[a] = 1  
        
        if Done == 1:
            Q_bar = x2[a] #predicted Q value
            #calculate the error signal
            e_n = R - Q_bar
            
            # Backpropagation: output layer -> hidden layer
            delta2 = x2 * (1-x2) * e_n * mask
            
            dW2 += np.outer(delta2, x1)
            dbias_W2 += delta2
            
            # Backpropagation: hidden layer -> input layer
            delta1 = x1 * (1-x1) * np.dot(W2.T, delta2)
            dW1 += np.outer(delta1,X)
            dbias_W1 += delta1
            
            W1 += eta * dW1
            W2 += eta * dW2
            bias_W1 += eta * dbias_W1
            bias_W2 += eta * dbias_W2
            
            R_save_Q[n] = np.copy(R) #saving variables
            N_moves_save_Q[n] = np.copy(i)
            print('episoid:',n+1,'; R:',R,'; Number of steps: ',i)
            break
            
            
        else:
          
            # Same neraul activation
            h1_tar = np.dot(W1, X_next) + bias_W1
            x1_tar = 1/(1+np.exp(-h1_tar))
            h2_tar = np.dot(W2, h1_tar) + bias_W2
            x2_tar = 1/(1+np.exp(-h2_tar))
            
            ##  Here's the different between Q-learning and SARSA
            #use epsilon greedy policy to find action at S'
            a2 = epsilon_greedy(epsilon_f, allowed_a_next, x2_tar)
            
            # Compute the error signal
            Q_bar = x2[a]
            Q_tar = R + gamma * x2_tar[a2] 
            e_n = Q_tar - Q_bar

            # Backpropagation: output layer -> hidden layer
            delta2 = x2 * (1-x2) * e_n * mask
            
            dW2 += np.outer(delta2, x1)
            dbias_W2 += delta2
            
            # Backpropagation: hidden layer -> input layer
            delta1 = x1*(1-x1) * np.dot(W2.T, delta2)
            dW1 += np.outer(delta1,X_next)
            dbias_W1 += delta1
            
            W1 += eta * dW1
            W2 += eta * dW2
            bias_W1 += eta * dbias_W1
            bias_W2 += eta * dbias_W2
            
            #update step number
            i += 1
        # Pass state values to the next step
        S = np.copy(S_next)
        X = np.copy(X_next)
        allowed_a = np.copy(allowed_a_next)

print('SARSA, Average reward:',np.mean(R_save_SARSA),'Number of steps: ',np.mean(N_moves_save_SARSA))

"""## Change rewards"""



print('Qlearning_Agent, Average reward:',np.mean(R_save),'Number of steps: ',np.mean(N_moves_save))

